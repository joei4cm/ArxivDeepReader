<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Evolution of AI Vision - Deformable Attention</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 320px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 400px;
            }
        }
        .gradient-text {
            background: linear-gradient(to right, #00F5D4, #9B5DE5);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            text-fill-color: transparent;
        }
    </style>
</head>
<body class="bg-gray-900 text-gray-200">

    <!-- 
        Infographic Plan:
        Narrative: Tell the story of how AI vision attention mechanisms evolved from inefficient global attention to "smart sparse" deformable attention, and then how these ideas extend into generative models like Stable Diffusion.
        Structure:
        1.  Header: Title and hook.
        2.  Section 1 (The Problem): Introduce the computational cost issue of early Vision Transformers (ViT).
        3.  Section 2 (The Evolution): Flowchart showing the progression to Deformable Attention.
        4.  Section 3 (The Breakthrough): Highlight the core idea of Deformable Attention.
        5.  Section 4 (How It Works): Technical breakdown of Deformable Attention.
        6.  Section 5 (The Champion): Head-to-head comparison of DAT vs. Swin.
        7.  Section 6 (The Generative Leap): New section explaining Stable Diffusion architecture.
        8.  Section 7 (The Legacy): Broader impact of these technologies.
        9.  Footer: Concluding remarks.

        Visualization Choices:
        -   Complexity Comparison, Attention Evolution, Computational Focus, How It Works, Training Speedup, DAT vs. Swin Charts, Broader Applications: All as before. (No SVG, No Mermaid JS)
        -   Stable Diffusion Architecture: Goal: Organize. Method: HTML/CSS with Tailwind to create a multi-step flowchart. Justification: Clearly explains the process flow from prompt to image without using prohibited SVG/Mermaid. (No SVG, No Mermaid JS)

        Confirmation: NEITHER Mermaid JS NOR SVG were used anywhere in this output. All visuals are created with Chart.js (Canvas) or styled HTML/CSS.
        Color Palette: "Energetic & Playful" (#00F5D4, #00CCBF, #9B5DE5, #F15BB5, #FEE440) on a dark background (#111827).
    -->

    <div class="container mx-auto p-4 md:p-8">

        <header class="text-center my-8 md:my-16">
            <h1 class="text-4xl md:text-6xl font-black tracking-tight leading-tight">The Evolution of the <span class="gradient-text">AI Eye</span></h1>
            <p class="mt-4 text-lg md:text-xl text-gray-400 max-w-3xl mx-auto">How AI learned to stop looking at everything, start focusing on what matters, and even begin to dream.</p>
        </header>

        <main class="space-y-16 md:space-y-24">

            <section id="problem">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">The Price of a Global Gaze</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">Early Vision Transformers (ViT) were powerful but incredibly inefficient. Their computational cost grew exponentially with image size, making high-resolution vision a huge challenge.</p>
                </div>
                <div class="bg-gray-800 rounded-xl shadow-2xl p-8 max-w-2xl mx-auto text-center">
                    <h3 class="text-2xl font-bold text-white">From Bad to Worse: Quadratic Complexity</h3>
                    <p class="text-gray-400 mt-2">If you doubled an image's resolution, the computation didn't just double, it exploded by 16 times! This is because every pixel must attend to every other pixel (N x N interactions).</p>
                    <div class="mt-6 flex items-center justify-center space-x-4 text-2xl md:text-4xl font-extrabold">
                        <span class="text-red-500">O(N¬≤)</span>
                        <span class="text-gray-500 text-4xl">‚Üí</span>
                        <span class="text-green-400">O(N)</span>
                    </div>
                    <p class="mt-2 text-gray-500">The goal was to get to a linear relationship.</p>
                </div>
            </section>

            <section id="evolution">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">The Journey to "Smart Focus"</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">To solve the efficiency problem, AI's "attention" mechanism went through a major evolution.</p>
                </div>
                <div class="grid grid-cols-1 md:grid-cols-3 gap-8 items-start">
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6 text-center flex flex-col h-full">
                        <div class="text-2xl font-bold">1. Global Attention</div>
                        <div class="text-sm font-semibold text-pink-400 mb-4">(The Brute Force Way)</div>
                        <p class="text-gray-400 flex-grow">Every pixel looks at every other pixel. Powerful, but incredibly slow and wasteful due to O(N¬≤) complexity.</p>
                        <div class="mt-4 text-5xl">üß†</div>
                    </div>
                    <div class="hidden md:flex items-center justify-center h-full text-5xl text-gray-600">‚Üí</div>
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6 text-center flex flex-col h-full">
                        <div class="text-2xl font-bold">2. Windowed Attention</div>
                        <div class="text-sm font-semibold text-yellow-400 mb-4">(The Rigid Grid Way)</div>
                        <p class="text-gray-400 flex-grow">Attention is limited to fixed, local windows. Efficient, but inflexible and "data-agnostic."</p>
                        <div class="mt-4 text-5xl">üñºÔ∏è</div>
                    </div>
                     <div class="hidden md:flex items-center justify-center h-full text-5xl text-gray-600">‚Üí</div>
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6 text-center flex flex-col h-full border-2 border-teal-400">
                        <div class="text-2xl font-bold">3. Deformable Attention</div>
                        <div class="text-sm font-semibold text-teal-400 mb-4">(The Smart Focus Way)</div>
                        <p class="text-gray-400 flex-grow">The AI learns where to look, sampling only a few key points based on content. Efficient AND flexible!</p>
                        <div class="mt-4 text-5xl">üéØ</div>
                    </div>
                </div>
            </section>

            <section id="breakthrough">
                 <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">The "Aha!" Moment</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">The breakthrough was to let the AI learn where to sample information, inspired by Deformable Convolutions. This had an immediate, dramatic impact.</p>
                </div>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6">
                        <h3 class="text-xl font-bold text-center mb-4">Focusing Computational Power</h3>
                        <p class="text-gray-400 text-center text-sm mb-4">Instead of processing millions of pixels, Deformable Attention focuses on a tiny, relevant subset.</p>
                        <div class="chart-container h-64 md:h-80">
                            <canvas id="focusChart"></canvas>
                        </div>
                        <p class="text-gray-500 text-xs text-center mt-2">Example: For each query, only 4 key points are sampled from a 64x64 (4096 total) feature map.</p>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6 flex flex-col items-center justify-center text-center">
                        <div class="text-7xl md:text-9xl font-black gradient-text">10x</div>
                        <h3 class="mt-4 text-2xl md:text-3xl font-bold">Faster Training</h3>
                        <p class="mt-2 text-gray-400">Deformable DETR trained 10 times faster than the original DETR, while also achieving better accuracy.</p>
                    </div>
                </div>
            </section>

            <section id="how-it-works">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">Under the Hood: How Smart Focus Works</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">It's not magic. It's a clever, multi-step process that allows the AI to learn where to look.</p>
                </div>
                <div class="max-w-5xl mx-auto grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6 flex flex-col items-center text-center h-full">
                        <div class="text-4xl font-bold text-purple-400 mb-4">1</div>
                        <div class="text-5xl mb-4">üìç</div>
                        <h4 class="text-lg font-bold mb-2">Define Reference Point</h4>
                        <p class="text-gray-400 text-sm flex-grow">For each query (a pixel or object query), the model starts with a base location or "reference point".</p>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6 flex flex-col items-center text-center h-full">
                        <div class="text-4xl font-bold text-purple-400 mb-4">2</div>
                        <div class="text-5xl mb-4">üß†</div>
                        <h4 class="text-lg font-bold mb-2">Predict Offsets</h4>
                        <p class="text-gray-400 text-sm flex-grow">A small neural network predicts a set of 2D offsets based on the query's content. This is the "learning where to look" step.</p>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6 flex flex-col items-center text-center h-full">
                        <div class="text-4xl font-bold text-purple-400 mb-4">3</div>
                        <div class="text-5xl mb-4">üß¨</div>
                        <h4 class="text-lg font-bold mb-2">Sample Features</h4>
                        <p class="text-gray-400 text-sm flex-grow">The model samples features from the new locations (reference + offset), using bilinear interpolation for sub-pixel accuracy.</p>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6 flex flex-col items-center text-center h-full">
                        <div class="text-4xl font-bold text-purple-400 mb-4">4</div>
                        <div class="text-5xl mb-4">‚ö°Ô∏è</div>
                        <h4 class="text-lg font-bold mb-2">Calculate Attention</h4>
                        <p class="text-gray-400 text-sm flex-grow">Attention is then calculated only on this tiny, relevant set of sampled points, achieving linear complexity.</p>
                    </div>
                </div>
            </section>

            <section id="showdown">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">Showdown: DAT vs. Swin</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">The new Deformable Attention Transformer (DAT) was benchmarked against the previous champion, Swin Transformer. The results were clear.</p>
                </div>
                <div class="grid grid-cols-1 lg:grid-cols-3 gap-8">
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6">
                        <h3 class="text-xl font-bold text-center mb-4">ImageNet Classification (Accuracy)</h3>
                        <div class="chart-container">
                            <canvas id="imagenetChart"></canvas>
                        </div>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6">
                        <h3 class="text-xl font-bold text-center mb-4">COCO Object Detection (Box AP)</h3>
                        <div class="chart-container">
                            <canvas id="cocoChart"></canvas>
                        </div>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-xl p-6">
                         <h3 class="text-xl font-bold text-center mb-4">ADE20K Segmentation (mIoU)</h3>
                        <div class="chart-container">
                            <canvas id="ade20kChart"></canvas>
                        </div>
                    </div>
                </div>
            </section>
            
            <section id="generative-leap">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">Beyond Recognition: The <span class="gradient-text">Generative Leap</span></h2>
                    <p class="mt-3 text-gray-400 max-w-3xl mx-auto">While DAT learns to *understand* images, another branch of AI uses similar Transformer concepts to *create* them. Models like Stable Diffusion can generate stunning images from just a text prompt.</p>
                </div>
                <div class="bg-gray-800/50 rounded-xl p-6 md:p-8">
                    <h3 class="text-2xl font-bold text-center mb-8">The Architecture of a Dream: How Stable Diffusion Works</h3>
                    <div class="grid grid-cols-1 md:grid-cols-5 gap-y-8 md:gap-x-4 items-start text-center">
                        
                        <!-- Column 1: Steps 1 & 2 -->
                        <div class="flex flex-col items-center gap-y-4">
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">‚úçÔ∏è</div>
                                <h4 class="font-bold mt-2">1. The Prompt</h4>
                                <p class="text-sm text-gray-400">A user writes a text description of the desired image.</p>
                            </div>
                            <div class="text-3xl text-gray-600">‚Üì</div>
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">üî°‚Üíüî¢</div>
                                <h4 class="font-bold mt-2">2. Text Encoder (CLIP)</h4>
                                <p class="text-sm text-gray-400">A <span class="font-bold text-teal-400">Transformer</span> model converts the text into numerical embeddings the AI can understand.</p>
                            </div>
                        </div>

                        <!-- Arrow 1 -> 2 -->
                        <div class="hidden md:flex justify-center items-center w-full h-full">
                            <div class="text-5xl text-gray-600 mt-[-120px]">‚Üí</div>
                        </div>

                        <!-- Column 2: Steps 3 & 4 -->
                        <div class="flex flex-col items-center gap-y-4">
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">üåÄ</div>
                                <h4 class="font-bold mt-2">3. Latent Diffusion</h4>
                                <p class="text-sm text-gray-400">The process starts with random noise in a compressed "latent space" for efficiency.</p>
                            </div>
                            <div class="text-3xl text-gray-600">‚Üì</div>
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">üßº</div>
                                <h4 class="font-bold mt-2">4. U-Net Denoising</h4>
                                <p class="text-sm text-gray-400">Guided by the text embedding, a U-Net model iteratively "cleans" the noise over many steps, forming a coherent latent image.</p>
                            </div>
                        </div>

                        <!-- Arrow 2 -> 3 -->
                        <div class="hidden md:flex justify-center items-center w-full h-full">
                            <div class="text-5xl text-gray-600 mt-[-120px]">‚Üí</div>
                        </div>

                        <!-- Column 3: Steps 5 & 6 -->
                        <div class="flex flex-col items-center gap-y-4">
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">‚ú®</div>
                                <h4 class="font-bold mt-2">5. VAE Decoder</h4>
                                <p class="text-sm text-gray-400">The clean latent image is fed into a Variational Autoencoder (VAE) decoder.</p>
                            </div>
                            <div class="text-3xl text-gray-600">‚Üì</div>
                            <div class="bg-gray-800 rounded-lg p-4 w-full h-full flex flex-col justify-center min-h-[200px]">
                                <div class="text-5xl">üñºÔ∏è</div>
                                <h4 class="font-bold mt-2">6. The Final Image</h4>
                                <p class="text-sm text-gray-400">The VAE translates the latent data back into pixel space, creating the final high-resolution image.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="legacy">
                <div class="text-center mb-12">
                    <h2 class="text-3xl md:text-4xl font-bold">A Legacy of Smart Vision</h2>
                    <p class="mt-3 text-gray-400 max-w-2xl mx-auto">The core idea of "learning where to look" and using Transformers for vision is so powerful, it's now being used to solve problems in many other fields.</p>
                </div>
                <div class="grid grid-cols-2 md:grid-cols-4 gap-4 md:gap-8 text-center">
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6">
                        <div class="text-5xl mb-3">üìà</div>
                        <h4 class="text-lg font-bold">Time Series</h4>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6">
                        <div class="text-5xl mb-3">üõ∞Ô∏è</div>
                        <h4 class="text-lg font-bold">Remote Sensing</h4>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6">
                        <div class="text-5xl mb-3">ü©∫</div>
                        <h4 class="text-lg font-bold">Medical Imaging</h4>
                    </div>
                    <div class="bg-gray-800 rounded-xl shadow-lg p-6">
                        <div class="text-5xl mb-3">üé¨</div>
                        <h4 class="text-lg font-bold">Visual Tracking</h4>
                    </div>
                </div>
            </section>

        </main>

        <footer class="text-center mt-16 md:mt-24 py-8 border-t border-gray-700">
            <p class="text-gray-500">From understanding to creation, advanced attention mechanisms represent a paradigm shift towards more efficient and intelligent AI systems.</p>
            <p class="text-sm text-gray-600 mt-2">Based on the research paper "Vision Transformer with Deformable Attention" (2106.09685) and Stable Diffusion architecture.</p>
        </footer>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const palette = {
                teal: '#00F5D4',
                purple: '#9B5DE5',
                pink: '#F15BB5',
                yellow: '#FEE440',
                gray: 'rgba(209, 213, 219, 0.5)'
            };

            function wrapLabel(str, maxWidth) {
                if (str.length <= maxWidth) {
                    return str;
                }
                const words = str.split(' ');
                let lines = [];
                let currentLine = words[0];

                for (let i = 1; i < words.length; i++) {
                    if ((currentLine + ' ' + words[i]).length > maxWidth) {
                        lines.push(currentLine);
                        currentLine = words[i];
                    } else {
                        currentLine += ' ' + words[i];
                    }
                }
                lines.push(currentLine);
                return lines;
            }
            
            const sharedTooltipConfig = {
                plugins: {
                    tooltip: {
                        callbacks: {
                            title: function(tooltipItems) {
                                const item = tooltipItems[0];
                                let label = item.chart.data.labels[item.dataIndex];
                                if (Array.isArray(label)) {
                                  return label.join(' ');
                                }
                                return label;
                            }
                        }
                    }
                }
            };

            const sharedChartOptions = {
                maintainAspectRatio: false,
                responsive: true,
                scales: {
                    y: {
                        beginAtZero: true,
                        grid: { color: palette.gray },
                        ticks: { color: 'white' }
                    },
                    x: {
                        grid: { display: false },
                        ticks: { color: 'white' }
                    }
                },
                plugins: {
                    legend: {
                        labels: { color: 'white' }
                    },
                    tooltip: sharedTooltipConfig.plugins.tooltip
                }
            };

            new Chart(document.getElementById('focusChart'), {
                type: 'doughnut',
                data: {
                    labels: ['Sampled Points', 'Ignored Points'],
                    datasets: [{
                        data: [4, 4092],
                        backgroundColor: [palette.teal, 'rgba(255, 255, 255, 0.1)'],
                        borderColor: [palette.teal, 'rgba(255, 255, 255, 0.1)'],
                        borderWidth: 1
                    }]
                },
                options: {
                    maintainAspectRatio: false,
                    responsive: true,
                    plugins: {
                        legend: {
                            position: 'bottom',
                            labels: { color: 'white' }
                        },
                        tooltip: sharedTooltipConfig.plugins.tooltip
                    }
                }
            });

            const benchmarkLabels = ['Tiny Model', 'Small Model', 'Base Model'];
            const swinColor = palette.pink;
            const datColor = palette.teal;

            new Chart(document.getElementById('imagenetChart'), {
                type: 'bar',
                data: {
                    labels: benchmarkLabels,
                    datasets: [
                        {
                            label: 'Swin',
                            data: [81.3, 83.0, 83.3],
                            backgroundColor: swinColor,
                        },
                        {
                            label: 'DAT',
                            data: [82.0, 83.6, 84.0],
                            backgroundColor: datColor,
                        }
                    ]
                },
                options: { ...sharedChartOptions, scales: { ...sharedChartOptions.scales, y: { ...sharedChartOptions.scales.y, min: 80 } } }
            });

            new Chart(document.getElementById('cocoChart'), {
                type: 'bar',
                data: {
                    labels: benchmarkLabels,
                    datasets: [
                        {
                            label: 'Swin',
                            data: [46.0, 48.5, 49.5],
                            backgroundColor: swinColor,
                        },
                        {
                            label: 'DAT',
                            data: [47.1, 49.4, 50.1],
                            backgroundColor: datColor,
                        }
                    ]
                },
                options: { ...sharedChartOptions, scales: { ...sharedChartOptions.scales, y: { ...sharedChartOptions.scales.y, min: 45 } } }
            });

            new Chart(document.getElementById('ade20kChart'), {
                type: 'bar',
                data: {
                    labels: benchmarkLabels,
                    datasets: [
                        {
                            label: 'Swin',
                            data: [47.6, 49.5, 49.7],
                            backgroundColor: swinColor,
                        },
                        {
                            label: 'DAT',
                            data: [48.6, 50.4, 50.9],
                            backgroundColor: datColor,
                        }
                    ]
                },
                options: { ...sharedChartOptions, scales: { ...sharedChartOptions.scales, y: { ...sharedChartOptions.scales.y, min: 47 } } }
            });
        });
    </script>

</body>
</html>
