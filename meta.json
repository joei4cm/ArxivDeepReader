{
  "papers": {
    "2412.19255": {
      "title": "解锁LLM效率：KV缓存优化技术深度解析 - Multi-matrix Factorization Attention: Breaking the KV Cache Bottleneck in LLMs",
      "description": "大型语言模型（LLM）的推理性能长期受限于一个核心瓶颈：键值（KV）缓存。本报告将深入探讨以多矩阵分解注意力（MFA）为代表的架构创新，如何从根本上解决这一挑战，推动AI进入一个更高效、更普及的新时代。",
      "category": "KV缓存优化",
      "categoryColor": "blue",
      "tags": [
        "架构创新",
        "内存优化",
        "性能提升"
      ],
      "tagColors": [
        "purple",
        "green",
        "orange"
      ],
      "gradient": "from-blue-600 to-purple-600",
      "folder": "2412.19255v2",
      "files": [
        {
          "name": "2412.19255v2.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2412.19255v2.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "LLM 缓存优化研究报告_.pdf",
          "type": "report",
          "priority": 2,
          "icon": "📊",
          "label": "研究报告"
        },
        {
          "name": "Step3-Application\\Step3-Model-Analysis.html",
          "type": "model_analysis",
          "priority": 2,
          "icon": "🧠",
          "label": "模型分析"
        },
        {
          "name": "Step3-Application\\Step3-Sys-Model.pdf",
          "type": "sys_model",
          "priority": 3,
          "icon": "⚙️",
          "label": "系统模型"
        },
        {
          "name": "Step3-Application\\Step3-MFA_MFA-KR解析.pdf",
          "type": "mfa_analysis",
          "priority": 3,
          "icon": "🔍",
          "label": "MFA-KR解析"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2412.19255"
    },
    "2507.11851": {
      "title": "信息图：揭示LLM的多令牌预测潜力",
      "description": "深入探讨 arXiv:2507.11851 论文，揭示大型语言模型（LLM）如何通过一次预测多个令牌来彻底改变推理速度。",
      "category": "推理加速",
      "categoryColor": "emerald",
      "tags": [
        "并行计算",
        "推测解码",
        "效率优化"
      ],
      "tagColors": [
        "blue",
        "red",
        "yellow"
      ],
      "gradient": "from-emerald-600 to-teal-600",
      "folder": "2507.11851v1",
      "files": [
        {
          "name": "2507.11851v1.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2507.11851v1.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "论文分析：原理、创新与应用_.pdf",
          "type": "analysis_pdf",
          "priority": 2,
          "icon": "📊",
          "label": "分析报告"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2507.11851"
    },
    "2405.16444": {
      "title": "CacheBlend: A Visual Deep Dive into High-Performance RAG",
      "description": "A visual deep dive into the technology that makes Retrieval-Augmented Generation fast, efficient, and commercially viable.",
      "category": "KV缓存优化",
      "categoryColor": "blue",
      "tags": [
        "架构创新",
        "内存优化",
        "性能提升"
      ],
      "tagColors": [
        "purple",
        "green",
        "orange"
      ],
      "gradient": "from-blue-600 to-purple-600",
      "folder": "2405.16444v3",
      "files": [
        {
          "name": "2405.1644v3_en.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2405.1644v3_cn.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2405.16444v3.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "深度解读CacheBlend：破解RAG推理瓶颈的缓存新策略.pdf",
          "type": "document",
          "priority": 3,
          "icon": "📄",
          "label": "PDF文档"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2405.16444"
    },
    "1706.03762": {
      "title": "解构革命: 'Attention Is All You Need' 可视化指南 (含MoE)",
      "description": "在Transformer横空出世前，语言模型的世界由循环神经网络（RNN）主导。它们像一个逐字阅读的读者，但存在两个致命缺陷：容易“遗忘”长距离信息，且天生的“串行”处理机制严重拖慢了训练速度，成为了AI发展的瓶颈。",
      "category": "KV缓存优化",
      "categoryColor": "blue",
      "tags": [
        "架构创新",
        "内存优化",
        "性能提升"
      ],
      "tagColors": [
        "purple",
        "green",
        "orange"
      ],
      "gradient": "from-blue-600 to-purple-600",
      "folder": "1706.03762v7",
      "files": [
        {
          "name": "1706.03762v7.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "Transformer论文分析与LLM入门指南_.pdf",
          "type": "analysis_pdf",
          "priority": 2,
          "icon": "📊",
          "label": "分析报告"
        },
        {
          "name": "1706.03762v7.pdf",
          "type": "document",
          "priority": 3,
          "icon": "📄",
          "label": "PDF文档"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/1706.03762"
    },
    "2403.03206": {
      "title": "Stable Diffusion 3 架构解析",
      "description": "一份详细介绍 Stable Diffusion 3 背后技术：多模态扩散变换器 (MMDiT) 的信息图。",
      "category": "推理加速",
      "categoryColor": "emerald",
      "tags": [
        "并行计算",
        "推测解码",
        "效率优化"
      ],
      "tagColors": [
        "blue",
        "red",
        "yellow"
      ],
      "gradient": "from-emerald-600 to-teal-600",
      "folder": "2403.03206v1",
      "files": [
        {
          "name": "2403.03206v1.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2403.03206v1.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "论文解读：自知识增强模型_.pdf",
          "type": "document",
          "priority": 3,
          "icon": "📄",
          "label": "PDF文档"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2403.03206"
    },
    "2505.09388": {
      "title": "Qwen3技术报告：信息图表",
      "description": "Qwen3提供从轻量级边缘设备到强大云服务器的全方位模型，涵盖密集型（Dense）和高效的混合专家（MoE）架构。",
      "category": "模型训练",
      "categoryColor": "purple",
      "tags": [
        "训练优化",
        "参数高效",
        "微调技术"
      ],
      "tagColors": [
        "blue",
        "green",
        "purple"
      ],
      "gradient": "from-purple-600 to-indigo-600",
      "folder": "2505.09388v1",
      "files": [
        {
          "name": "2505.09388v1.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2505.09388v1.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "论文分析：Qwen3模型规格与变种_.pdf",
          "type": "analysis_pdf",
          "priority": 2,
          "icon": "📊",
          "label": "分析报告"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2505.09388"
    },
    "2106.09685": {
      "title": "信息图：LoRA革命 - AI定制的全新时代",
      "description": "在LoRA出现前，想让一个大模型学会特定任务（微调），就像要重修一座大厦——成本高昂，普通人难以承受。",
      "category": "模型训练",
      "categoryColor": "purple",
      "tags": [
        "训练优化",
        "参数高效",
        "微调技术"
      ],
      "tagColors": [
        "blue",
        "green",
        "purple"
      ],
      "gradient": "from-purple-600 to-indigo-600",
      "folder": "2106.09685v2",
      "files": [
        {
          "name": "2106.09685v2.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "2106.09685v2.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "原文PDF"
        },
        {
          "name": "LoRA论文技术分析报告_.pdf",
          "type": "report",
          "priority": 2,
          "icon": "📊",
          "label": "研究报告"
        }
      ],
      "arxivUrl": "https://arxiv.org/abs/2106.09685"
    },
    "Kimi-K2": {
      "title": "Kimi-K2 技术报告：信息图表",
      "description": "Kimi-K2 是月之暗面推出的新一代大型语言模型，采用创新的混合专家（MoE）架构，在保持高性能的同时显著提升了推理效率。",
      "category": "模型架构",
      "categoryColor": "indigo",
      "tags": [
        "MoE架构",
        "推理优化",
        "技术报告"
      ],
      "tagColors": [
        "indigo",
        "orange",
        "gray"
      ],
      "gradient": "from-indigo-600 to-purple-600",
      "folder": "Kimi-K2",
      "files": [
        {
          "name": "Kimi-K2_Infographic.html",
          "type": "analysis",
          "priority": 1,
          "icon": "📖",
          "label": "深度解析"
        },
        {
          "name": "kimi-k2_tech_report.pdf",
          "type": "original",
          "priority": 2,
          "icon": "📄",
          "label": "技术报告"
        },
        {
          "name": "Kimi-K2 MoE Design Analysis_.pdf",
          "type": "analysis_pdf",
          "priority": 2,
          "icon": "📊",
          "label": "架构分析"
        },
        {
          "name": "Kimi-K2 与 DeepSeek V3 对比_.pdf",
          "type": "comparison",
          "priority": 3,
          "icon": "⚖️",
          "label": "对比分析"
        }
      ],
      "url": "https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf"
    }
  },
  "statistics": {
    "totalPapers": 8,
    "totalDocuments": 29
  },
  "lastUpdated": "2025-07-27T21:35:09.546796Z",
  "version": "1.0.0"
}